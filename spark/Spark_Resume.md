# ğŸ“š RÃ©sumÃ© Complet Apache Spark - Guide Junior

> **Objectif** : Tout ce qu'un IngÃ©nieur Support & IntÃ©gration Junior Big Data doit savoir sur Spark

---

## Table des matiÃ¨res

1. [Spark vs MapReduce](#1--spark-vs-mapreduce)
2. [Architecture Spark](#2-ï¸-architecture-spark)
3. [RDD, DataFrame, Dataset](#3--rdd-dataframe-dataset)
4. [Lazy Evaluation](#4--lazy-evaluation)
5. [Transformations vs Actions](#5--transformations-vs-actions)
6. [DAG (Directed Acyclic Graph)](#6--dag-directed-acyclic-graph)
7. [Jobs â†’ Stages â†’ Tasks](#7--jobs--stages--tasks)
8. [Shuffle](#8--shuffle)
9. [Partitions](#9--partitions)
10. [Cache et Persist](#10--cache-et-persist)
11. [Data Skew](#11-ï¸-data-skew)
12. [Broadcast Variables](#12--broadcast-variables)
13. [Formats de fichiers](#13--formats-de-fichiers)
14. [Spark UI](#14-ï¸-spark-ui)
15. [Configurations](#15-ï¸-configurations)
16. [Commandes spark-submit](#16--commandes-spark-submit)
17. [Erreurs courantes](#17--erreurs-courantes)
18. [Checklist Entretien Junior](#18--checklist-entretien-junior)

---

## 1. âš¡ Spark vs MapReduce

### Pourquoi Spark remplace MapReduce ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MapReduce vs Spark                                    â”‚
â”‚                                                                          â”‚
â”‚  MAPREDUCE:                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Map  â”‚ â”€â–º â”‚DISQUEâ”‚ â”€â–º â”‚Reduceâ”‚ â”€â–º â”‚DISQUEâ”‚ â”€â–º â”‚ Map  â”‚ â”€â–º ...       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                 â–²                        â–²                               â”‚
â”‚                 â”‚                        â”‚                               â”‚
â”‚           Ã‰criture I/O             Ã‰criture I/O                         â”‚
â”‚              LENT!                    LENT!                              â”‚
â”‚                                                                          â”‚
â”‚  SPARK:                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Op 1 â”‚ â”€â–º â”‚ RAM  â”‚ â”€â–º â”‚ Op 2 â”‚ â”€â–º â”‚ RAM  â”‚ â”€â–º â”‚ Op 3 â”‚ â”€â–º ...       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                 â–²                        â–²                               â”‚
â”‚                 â”‚                        â”‚                               â”‚
â”‚           En mÃ©moire               En mÃ©moire                            â”‚
â”‚             RAPIDE!                  RAPIDE!                             â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison

| CritÃ¨re | MapReduce | Spark |
|---------|-----------|-------|
| **Stockage intermÃ©diaire** | Disque (HDFS) | RAM (mÃ©moire) |
| **Vitesse** | Lent | 10-100x plus rapide |
| **FacilitÃ©** | Code verbeux | API simple (Python, Scala) |
| **Cas d'usage** | Batch uniquement | Batch + Streaming + ML |
| **ItÃ©rations** | Mauvais (relit disque) | Excellent (garde en RAM) |

### Pourquoi Spark est plus rapide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AVANTAGES DE SPARK                                     â”‚
â”‚                                                                          â”‚
â”‚  1. IN-MEMORY PROCESSING                                                â”‚
â”‚     â†’ DonnÃ©es gardÃ©es en RAM entre les opÃ©rations                       â”‚
â”‚                                                                          â”‚
â”‚  2. LAZY EVALUATION                                                      â”‚
â”‚     â†’ Optimise le plan avant d'exÃ©cuter                                 â”‚
â”‚                                                                          â”‚
â”‚  3. DAG OPTIMIZER                                                        â”‚
â”‚     â†’ Fusionne les opÃ©rations, Ã©limine les Ã©tapes inutiles             â”‚
â”‚                                                                          â”‚
â”‚  4. CATALYST OPTIMIZER (pour DataFrame/SQL)                             â”‚
â”‚     â†’ Optimisation automatique des requÃªtes                             â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ğŸ—ï¸ Architecture Spark

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLUSTER SPARK                                     â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         DRIVER                                   â”‚    â”‚
â”‚  â”‚                    (Programme principal)                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚ SparkContext â”‚  â”‚  DAG         â”‚  â”‚  Task        â”‚          â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  Scheduler   â”‚  â”‚  Scheduler   â”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚                                â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    CLUSTER MANAGER                               â”‚    â”‚
â”‚  â”‚               (YARN / Mesos / Kubernetes / Standalone)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                         â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚          â”‚                     â”‚                     â”‚                  â”‚
â”‚          â–¼                     â–¼                     â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ WORKER NODE  â”‚      â”‚ WORKER NODE  â”‚      â”‚ WORKER NODE  â”‚          â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚          â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚
â”‚  â”‚ â”‚ EXECUTOR â”‚ â”‚      â”‚ â”‚ EXECUTOR â”‚ â”‚      â”‚ â”‚ EXECUTOR â”‚ â”‚          â”‚
â”‚  â”‚ â”‚          â”‚ â”‚      â”‚ â”‚          â”‚ â”‚      â”‚ â”‚          â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚      â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚      â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚      â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚      â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚      â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚      â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚      â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚      â”‚ â”‚ â”‚ Task â”‚ â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚      â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚      â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â”‚Cache â”‚ â”‚ â”‚      â”‚ â”‚ â”‚Cache â”‚ â”‚ â”‚      â”‚ â”‚ â”‚Cache â”‚ â”‚ â”‚          â”‚
â”‚  â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚      â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚      â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚          â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

| Composant | Description | RÃ´le |
|-----------|-------------|------|
| **Driver** | Programme principal | CrÃ©e SparkContext, planifie les jobs, coordonne |
| **SparkContext** | Point d'entrÃ©e | Connexion au cluster |
| **Cluster Manager** | Gestionnaire de ressources | Alloue les ressources (YARN, Mesos, K8s) |
| **Worker Node** | Machine physique/VM | HÃ©berge les executors |
| **Executor** | Processus JVM | ExÃ©cute les tasks, stocke les donnÃ©es en cache |
| **Task** | UnitÃ© de travail | Traite une partition |

### DiffÃ©rence Worker vs Executor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  WORKER NODE (Machine)              EXECUTOR (Processus)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                      â”‚          â”‚                      â”‚            â”‚
â”‚  â”‚  C'est la MACHINE    â”‚          â”‚  C'est le PROCESSUS  â”‚            â”‚
â”‚  â”‚  physique ou VM      â”‚          â”‚  JVM qui tourne      â”‚            â”‚
â”‚  â”‚                      â”‚          â”‚  sur le worker       â”‚            â”‚
â”‚  â”‚  Peut avoir          â”‚          â”‚                      â”‚            â”‚
â”‚  â”‚  PLUSIEURS           â”‚          â”‚  ExÃ©cute les TASKS   â”‚            â”‚
â”‚  â”‚  executors           â”‚          â”‚  Stocke le CACHE     â”‚            â”‚
â”‚  â”‚                      â”‚          â”‚                      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  Analogie:                                                               â”‚
â”‚  Worker = Bureau (le lieu)                                               â”‚
â”‚  Executor = EmployÃ© (qui travaille dans le bureau)                      â”‚
â”‚  Task = TÃ¢che assignÃ©e Ã  l'employÃ©                                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ğŸ“Š RDD, DataFrame, Dataset

### Ã‰volution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  RDD (2011)           DataFrame (2013)         Dataset (2015)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ [Row1]      â”‚      â”‚ | nom  | age |  â”‚      â”‚ | nom  | age |  â”‚      â”‚
â”‚  â”‚ [Row2]      â”‚      â”‚ |------|-----|  â”‚      â”‚ |------|-----|  â”‚      â”‚
â”‚  â”‚ [Row3]      â”‚      â”‚ | Ali  | 25  |  â”‚      â”‚ | Ali  | 25  |  â”‚      â”‚
â”‚  â”‚ ...         â”‚      â”‚ | Sara | 30  |  â”‚      â”‚ | Sara | 30  |  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                          â”‚
â”‚  Bas niveau           Haut niveau              Haut niveau + TypÃ©       â”‚
â”‚  Pas de schÃ©ma        SchÃ©ma + Catalyst        SchÃ©ma + Catalyst        â”‚
â”‚  Pas d'optimisation   OptimisÃ©                 OptimisÃ© + Type-safe     â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison

| CritÃ¨re | RDD | DataFrame | Dataset |
|---------|-----|-----------|---------|
| **Niveau** | Bas | Haut | Haut |
| **SchÃ©ma** | Non | Oui | Oui |
| **Optimisation Catalyst** | Non | Oui | Oui |
| **Type-safe** | Oui | Non | Oui |
| **Langages** | Scala, Java, Python | Scala, Java, Python, R | Scala, Java |
| **Performance** | Moins bon | Meilleur | Meilleur |

### Quand utiliser quoi ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       QUAND UTILISER QUOI ?                              â”‚
â”‚                                                                          â”‚
â”‚  DataFrame (recommandÃ©):                                                â”‚
â”‚  âœ… 90% des cas                                                         â”‚
â”‚  âœ… RequÃªtes SQL-like                                                   â”‚
â”‚  âœ… Optimisations automatiques                                          â”‚
â”‚                                                                          â”‚
â”‚  RDD:                                                                    â”‚
â”‚  âœ… ContrÃ´le fin sur les donnÃ©es                                        â”‚
â”‚  âœ… DonnÃ©es non structurÃ©es                                             â”‚
â”‚  âœ… OpÃ©rations bas niveau                                               â”‚
â”‚                                                                          â”‚
â”‚  Dataset:                                                                â”‚
â”‚  âœ… Besoin de type-safety (Scala/Java)                                  â”‚
â”‚  âœ… Mix entre RDD et DataFrame                                          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. ğŸ’¤ Lazy Evaluation

### Concept

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LAZY EVALUATION                                   â”‚
â”‚                                                                          â”‚
â”‚  # Ces lignes NE S'EXÃ‰CUTENT PAS                                        â”‚
â”‚  df2 = df.filter(df.age > 25)      # Transformation â†’ LAZY              â”‚
â”‚  df3 = df2.select("name")          # Transformation â†’ LAZY              â”‚
â”‚  df4 = df3.groupBy("city")         # Transformation â†’ LAZY              â”‚
â”‚                                                                          â”‚
â”‚  # Spark construit juste le PLAN (DAG)                                  â”‚
â”‚                                                                          â”‚
â”‚  # Maintenant Ã§a S'EXÃ‰CUTE !                                            â”‚
â”‚  df4.count()                        # Action â†’ DÃ‰CLENCHE TOUT           â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visualisation

```
CODE:                               CE QUI SE PASSE:

df.filter(...)    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    Spark note: "filter Ã  faire"
      â”‚                                         â”‚
      â–¼                                         â–¼
df.select(...)    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    Spark note: "select Ã  faire"
      â”‚                                         â”‚
      â–¼                                         â–¼
df.groupBy(...)   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    Spark note: "groupBy Ã  faire"
      â”‚                                         â”‚
      â–¼                                         â–¼
df.count()        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    Spark: "OK, maintenant j'exÃ©cute tout!"
                                    â†’ Optimise le plan
                                    â†’ ExÃ©cute filter+select+groupBy+count
```

### Pourquoi c'est utile ?

| Avantage | Explication |
|----------|-------------|
| **Optimisation** | Spark optimise le plan avant d'exÃ©cuter |
| **Fusion d'opÃ©rations** | Plusieurs opÃ©rations combinÃ©es en une |
| **Ã‰limination** | Colonnes/donnÃ©es inutiles jamais lues |
| **EfficacitÃ©** | Moins de passes sur les donnÃ©es |

---

## 5. ğŸ”„ Transformations vs Actions

### DiffÃ©rence fondamentale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  TRANSFORMATION                        ACTION                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                    â”‚               â”‚                    â”‚            â”‚
â”‚  â”‚  â†’ CrÃ©e un NOUVEAU â”‚               â”‚  â†’ DÃ‰CLENCHE       â”‚            â”‚
â”‚  â”‚    DataFrame       â”‚               â”‚    l'exÃ©cution     â”‚            â”‚
â”‚  â”‚                    â”‚               â”‚                    â”‚            â”‚
â”‚  â”‚  â†’ LAZY            â”‚               â”‚  â†’ Retourne        â”‚            â”‚
â”‚  â”‚    (pas exÃ©cutÃ©)   â”‚               â”‚    rÃ©sultat au     â”‚            â”‚
â”‚  â”‚                    â”‚               â”‚    driver          â”‚            â”‚
â”‚  â”‚  â†’ Construit le    â”‚               â”‚                    â”‚            â”‚
â”‚  â”‚    DAG             â”‚               â”‚  â†’ Ã‰crit sur       â”‚            â”‚
â”‚  â”‚                    â”‚               â”‚    stockage        â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Liste des opÃ©rations

| Transformations (Lazy) | Actions (DÃ©clenche) |
|------------------------|---------------------|
| `filter()` | `count()` |
| `select()` | `show()` |
| `map()` | `collect()` |
| `flatMap()` | `take(n)` |
| `groupBy()` | `first()` |
| `join()` | `write()` |
| `orderBy()` / `sort()` | `foreach()` |
| `distinct()` | `reduce()` |
| `withColumn()` | `saveAsTable()` |
| `drop()` | `toPandas()` |
| `union()` | `head()` |
| `repartition()` | `describe()` |

### Narrow vs Wide Transformations

```
NARROW (pas de shuffle = rapide):
â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
â”‚ P0  â”‚ â”€â”€â–º â”‚ P0' â”‚   Chaque partition travaille SEULE
â”œâ”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¤
â”‚ P1  â”‚ â”€â”€â–º â”‚ P1' â”‚   Pas de transfert rÃ©seau
â”œâ”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”¤
â”‚ P2  â”‚ â”€â”€â–º â”‚ P2' â”‚
â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜

Exemples: filter, select, map, flatMap, withColumn


WIDE (shuffle = lent):
â”Œâ”€â”€â”€â”€â”€â”     
â”‚ P0  â”‚ â”€â”€â”¬â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”
â”œâ”€â”€â”€â”€â”€â”¤   â”‚    â”‚ P0' â”‚   DonnÃ©es MÃ‰LANGÃ‰ES entre partitions
â”‚ P1  â”‚ â”€â”€â”¼â”€â”€â–º â”œâ”€â”€â”€â”€â”€â”¤
â”œâ”€â”€â”€â”€â”€â”¤   â”‚    â”‚ P1' â”‚   Transfert rÃ©seau
â”‚ P2  â”‚ â”€â”€â”´â”€â”€â–º â””â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”˜     

Exemples: groupBy, join, orderBy, distinct, repartition
```

| Type | Shuffle ? | Exemples | Performance |
|------|-----------|----------|-------------|
| **Narrow** | âŒ Non | `filter`, `select`, `map`, `withColumn` | âš¡ Rapide |
| **Wide** | âœ… Oui | `groupBy`, `join`, `orderBy`, `distinct` | ğŸ¢ Lent |

---

## 6. ğŸ“ˆ DAG (Directed Acyclic Graph)

### C'est quoi ?

Le DAG est le **plan d'exÃ©cution** que Spark construit avant de lancer le travail.

```
CODE:
df.filter(age > 25).select("name").groupBy("city").count()

DAG CONSTRUIT:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Lire fichier â”‚  â† Ã‰tape 1
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ filter(age>25)â”‚  â† Ã‰tape 2
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ select(name) â”‚  â† Ã‰tape 3
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚groupBy(city) â”‚  â† Ã‰tape 4 (SHUFFLE ici!)
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    count()   â”‚  â† Ã‰tape 5 (ACTION = lance tout)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimisation du DAG

```
AVANT optimisation:              APRÃˆS optimisation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ filter â”‚                       â”‚ filter â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â”‚   +    â”‚
    â”‚                            â”‚ select â”‚  â† FusionnÃ©s
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”                       â”‚   +    â”‚
â”‚ select â”‚                       â”‚ filter â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ filter â”‚                       â”‚groupBy â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚groupBy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4 Ã©tapes                         2 Ã©tapes (plus rapide!)
```

### Visualiser dans Spark UI

```
http://localhost:4040 â†’ Jobs â†’ DAG Visualization

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK UI - DAG                                      â”‚
â”‚                                                                          â”‚
â”‚   Stage 0                    Stage 1                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚ read    â”‚               â”‚ groupBy â”‚                                 â”‚
â”‚  â”‚ filter  â”‚ â”€â”€SHUFFLEâ”€â”€â–º  â”‚ count   â”‚                                 â”‚
â”‚  â”‚ select  â”‚               â”‚         â”‚                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                                                                          â”‚
â”‚  OpÃ©rations NARROW          Nouvelle stage aprÃ¨s SHUFFLE                â”‚
â”‚  groupÃ©es ensemble                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. ğŸ“‹ Jobs â†’ Stages â†’ Tasks

### HiÃ©rarchie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      HIÃ‰RARCHIE SPARK                                    â”‚
â”‚                                                                          â”‚
â”‚  APPLICATION                                                            â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â”œâ”€â”€ JOB 0 (dÃ©clenchÃ© par count())                                 â”‚
â”‚       â”‚      â”‚                                                           â”‚
â”‚       â”‚      â”œâ”€â”€ Stage 0 (avant shuffle)                                â”‚
â”‚       â”‚      â”‚      â”œâ”€â”€ Task 0 (partition 0)                            â”‚
â”‚       â”‚      â”‚      â”œâ”€â”€ Task 1 (partition 1)                            â”‚
â”‚       â”‚      â”‚      â””â”€â”€ Task 2 (partition 2)                            â”‚
â”‚       â”‚      â”‚                                                           â”‚
â”‚       â”‚      â””â”€â”€ Stage 1 (aprÃ¨s shuffle)                                â”‚
â”‚       â”‚             â”œâ”€â”€ Task 0                                           â”‚
â”‚       â”‚             â””â”€â”€ Task 1                                           â”‚
â”‚       â”‚                                                                  â”‚
â”‚       â””â”€â”€ JOB 1 (dÃ©clenchÃ© par show())                                  â”‚
â”‚              â””â”€â”€ ...                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RÃ¨gles

| Niveau | CrÃ©Ã© par | Contient |
|--------|----------|----------|
| **Job** | Chaque ACTION (count, show, write) | Plusieurs stages |
| **Stage** | SÃ©parÃ© par les SHUFFLES | Plusieurs tasks |
| **Task** | 1 par PARTITION | Travail sur 1 partition |

### Exemple concret

```python
# Code
df = spark.read.csv("ventes.csv")        # 3 partitions
df2 = df.filter(df.montant > 100)        # narrow
df3 = df2.select("client", "ville")      # narrow
df4 = df3.groupBy("ville").count()       # wide (SHUFFLE)
df5 = df4.orderBy("count")               # wide (SHUFFLE)
df5.show()                               # ACTION
```

```
RÃ‰SULTAT:

show() â†’ JOB 0
           â”‚
           â”œâ”€â”€ STAGE 0: read + filter + select
           â”‚      â”œâ”€â”€ Task 0 (partition 0)
           â”‚      â”œâ”€â”€ Task 1 (partition 1)
           â”‚      â””â”€â”€ Task 2 (partition 2)
           â”‚            â”‚
           â”‚       [SHUFFLE - groupBy]
           â”‚            â”‚
           â”œâ”€â”€ STAGE 1: groupBy + count
           â”‚      â”œâ”€â”€ Task 0
           â”‚      â””â”€â”€ Task 1
           â”‚            â”‚
           â”‚       [SHUFFLE - orderBy]
           â”‚            â”‚
           â””â”€â”€ STAGE 2: orderBy
                  â””â”€â”€ Task 0

TOTAL: 1 Job, 3 Stages, 6 Tasks
```

### RÃ¨gles Ã  retenir

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RÃˆGLES CLÃ‰S                                      â”‚
â”‚                                                                          â”‚
â”‚  1. 1 ACTION = 1 JOB                                                    â”‚
â”‚     df.count()  â†’ Job 0                                                 â”‚
â”‚     df.show()   â†’ Job 1                                                 â”‚
â”‚                                                                          â”‚
â”‚  2. 1 SHUFFLE = nouvelle STAGE                                          â”‚
â”‚     groupBy, join, orderBy â†’ nouvelle stage                             â”‚
â”‚                                                                          â”‚
â”‚  3. 1 PARTITION = 1 TASK (par stage)                                    â”‚
â”‚     10 partitions â†’ 10 tasks parallÃ¨les                                 â”‚
â”‚                                                                          â”‚
â”‚  4. NARROW transformations = mÃªme stage                                  â”‚
â”‚     filter + select + map â†’ groupÃ©s dans 1 stage                        â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. ğŸ”€ Shuffle

### C'est quoi ?

Le **shuffle** c'est quand Spark doit **redistribuer les donnÃ©es** entre les executors via le rÃ©seau.

### Exemple : groupBy

```
DONNÃ‰ES DE DÃ‰PART (3 partitions sur 3 executors):

Executor 1 (Partition 0):     Executor 2 (Partition 1):     Executor 3 (Partition 2):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {city: Tunis, v: 10}â”‚       â”‚ {city: Sfax,  v: 20}â”‚       â”‚ {city: Tunis, v: 50}â”‚
â”‚ {city: Sfax,  v: 15}â”‚       â”‚ {city: Tunis, v: 30}â”‚       â”‚ {city: Sfax,  v: 60}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        df.groupBy("city").sum("v")
                                    â”‚
                                    â–¼
                              SHUFFLE !
            (toutes les donnÃ©es de mÃªme ville doivent aller ensemble)
                                    â”‚
                                    â–¼

APRÃˆS SHUFFLE:

Executor 1:                   Executor 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tunis: 10, 30, 50   â”‚       â”‚ Sfax: 15, 20, 60    â”‚
â”‚ SUM = 90            â”‚       â”‚ SUM = 95            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi c'est LENT ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COÃ›T DU SHUFFLE                                       â”‚
â”‚                                                                          â”‚
â”‚  1. Ã‰CRITURE DISQUE (Shuffle Write)                                     â”‚
â”‚     â†’ Executor Ã©crit ses donnÃ©es sur disque local                        â”‚
â”‚                                                                          â”‚
â”‚  2. TRANSFERT RÃ‰SEAU                                                    â”‚
â”‚     â†’ DonnÃ©es envoyÃ©es entre machines                                    â”‚
â”‚                                                                          â”‚
â”‚  3. LECTURE DISQUE (Shuffle Read)                                       â”‚
â”‚     â†’ Executor lit les donnÃ©es reÃ§ues                                    â”‚
â”‚                                                                          â”‚
â”‚  4. SÃ‰RIALISATION / DÃ‰SÃ‰RIALISATION                                     â”‚
â”‚     â†’ Conversion objets â†’ bytes â†’ objets                                â”‚
â”‚                                                                          â”‚
â”‚  = TRÃˆS LENT comparÃ© aux opÃ©rations en mÃ©moire !                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### OpÃ©rations qui causent un shuffle

| OpÃ©ration | Pourquoi shuffle ? |
|-----------|-------------------|
| `groupBy()` | Regrouper par clÃ© |
| `join()` | Associer 2 tables par clÃ© |
| `distinct()` | Ã‰liminer doublons (doit comparer tout) |
| `orderBy()` / `sort()` | Trier globalement |
| `repartition()` | Redistribuer explicitement |
| `coalesce()` avec shuffle | Si augmentation de partitions |

### OpÃ©rations SANS shuffle

| OpÃ©ration | Pourquoi pas de shuffle ? |
|-----------|--------------------------|
| `filter()` | Chaque partition filtre ses donnÃ©es |
| `select()` | Chaque partition sÃ©lectionne ses colonnes |
| `map()` | Transformation locale |
| `withColumn()` | Ajout/modif colonne locale |
| `coalesce()` sans shuffle | Fusion locale de partitions |

### Comment optimiser ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RÃ‰DUIRE LES SHUFFLES                                    â”‚
â”‚                                                                          â”‚
â”‚  1. Utiliser reduceByKey au lieu de groupByKey                          â”‚
â”‚     â†’ AgrÃ©gation locale avant shuffle                                   â”‚
â”‚                                                                          â”‚
â”‚  2. Broadcast Join pour petites tables                                   â”‚
â”‚     â†’ Ã‰vite le shuffle de la grande table                               â”‚
â”‚                                                                          â”‚
â”‚  3. Partitionner par clÃ© de join                                         â”‚
â”‚     â†’ DonnÃ©es dÃ©jÃ  colocalisÃ©es                                         â”‚
â”‚                                                                          â”‚
â”‚  4. Ã‰viter distinct() si possible                                        â”‚
â”‚     â†’ Utiliser dropDuplicates() sur colonnes spÃ©cifiques                â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. ğŸ“¦ Partitions

### C'est quoi ?

Une **partition** c'est un **morceau** de donnÃ©es. Spark divise les donnÃ©es en partitions pour les traiter en parallÃ¨le.

```
FICHIER 1 GB:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DONNÃ‰ES (1 GB)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Spark divise en partitions
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Partition 0â”‚  â”‚Partition 1â”‚  â”‚Partition 2â”‚  â”‚Partition 3â”‚
â”‚  250 MB   â”‚  â”‚  250 MB   â”‚  â”‚  250 MB   â”‚  â”‚  250 MB   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚              â”‚              â”‚
      â–¼              â–¼              â–¼              â–¼
  Executor 1     Executor 2    Executor 3     Executor 4
  (Task 0)       (Task 1)      (Task 2)       (Task 3)

= 4 executors travaillent EN PARALLÃˆLE !
```

### RÃ¨gle fondamentale

```
1 Partition = 1 Task = 1 Core (Ã  un moment donnÃ©)
```

### Taille recommandÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RÃˆGLE DE TAILLE DES PARTITIONS                              â”‚
â”‚                                                                          â”‚
â”‚  IdÃ©al : 128 MB - 1 GB par partition                                    â”‚
â”‚                                                                          â”‚
â”‚  Trop petit (< 50 MB):                                                  â”‚
â”‚  â†’ Trop de tasks â†’ overhead de scheduling                                â”‚
â”‚  â†’ Spark passe plus de temps Ã  gÃ©rer qu'Ã  calculer                      â”‚
â”‚                                                                          â”‚
â”‚  Trop grand (> 2 GB):                                                   â”‚
â”‚  â†’ Risque OOM (Out Of Memory)                                            â”‚
â”‚  â†’ Moins de parallÃ©lisme                                                 â”‚
â”‚  â†’ Garbage Collection lent                                               â”‚
â”‚                                                                          â”‚
â”‚  Formule simple:                                                         â”‚
â”‚  nb_partitions = taille_donnÃ©es / 128 MB                                â”‚
â”‚  ou                                                                      â”‚
â”‚  nb_partitions = nb_cores Ã— 2 Ã  4                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### repartition vs coalesce

```python
# Voir le nombre de partitions
df.rdd.getNumPartitions()  # â†’ 4

# REPARTITION: redistribue avec shuffle (peut augmenter ou diminuer)
df2 = df.repartition(10)  # Shuffle complet

# COALESCE: fusionne sans shuffle (peut seulement diminuer)
df3 = df.coalesce(2)  # Pas de shuffle, plus rapide
```

```
repartition(n):                     coalesce(n):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P0 â”‚ P1 â”‚ P2â”‚                     â”‚ P0 â”‚ P1 â”‚ P2â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                   â”‚
      â–¼ SHUFFLE                           â–¼ PAS DE SHUFFLE
      â”‚                                   â”‚
      â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚P0â”‚P1â”‚P2â”‚P3â”‚P4â”‚P5â”‚                 â”‚  P0 â”‚ P1  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Peut AUGMENTER                      Peut seulement DIMINUER
ou diminuer                         (plus rapide)
```

| MÃ©thode | Shuffle | Direction | Usage |
|---------|---------|-----------|-------|
| `repartition(n)` | âœ… Oui | â†‘â†“ Augmenter ou diminuer | Redistribuer uniformÃ©ment |
| `coalesce(n)` | âŒ Non | â†“ Diminuer seulement | RÃ©duire avant Ã©criture |

---

## 10. ğŸ’¾ Cache et Persist

### Le problÃ¨me

Par dÃ©faut, Spark **recalcule** tout Ã  chaque action :

```python
df = spark.read.csv("data.csv")
df2 = df.filter(df.age > 25).select("name", "city")

# Action 1 : Spark lit CSV + filter + select
df2.count()  

# Action 2 : Spark RELIT CSV + filter + select (encore!)
df2.show()

# Action 3 : Spark RELIT CSV + filter + select (encore!!)
df2.write.csv("output")

# = 3 fois le mÃªme travail !
```

### La solution : cache()

```python
df = spark.read.csv("data.csv")
df2 = df.filter(df.age > 25).select("name", "city")

# GARDE EN MÃ‰MOIRE
df2.cache()

# Action 1 : Lit CSV + filter + select + STOCKE en RAM
df2.count()  

# Action 2 : Lit depuis RAM (instantanÃ©!)
df2.show()

# Action 3 : Lit depuis RAM (instantanÃ©!)
df2.write.csv("output")

# = 1 seule lecture, 3 utilisations rapides !
```

### Visualisation

```
SANS CACHE:                              AVEC CACHE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ count()  â”‚ â”‚  show()  â”‚ â”‚ write()  â”‚   â”‚ count()  â”‚ â”‚  show()  â”‚ â”‚ write()  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚              â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”                       â”‚
  â”‚filterâ”‚     â”‚filterâ”‚     â”‚filterâ”‚                       â–¼
  â””â”€â”€â”¬â”€â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”€â”˜                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚            â”‚            â”‚                    â”‚    CACHE    â”‚
     â–¼            â–¼            â–¼                    â”‚   (RAM)     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  â”‚ read â”‚     â”‚ read â”‚     â”‚ read â”‚                       â”‚
  â”‚ CSV  â”‚     â”‚ CSV  â”‚     â”‚ CSV  â”‚                       â–¼ (1 seule fois)
  â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜                   â”Œâ”€â”€â”€â”€â”€â”€â”
                                                       â”‚filterâ”‚
= 3 lectures !                                         â””â”€â”€â”¬â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
                                                       â”Œâ”€â”€â”€â”€â”€â”€â”
                                                       â”‚ read â”‚
                                                       â””â”€â”€â”€â”€â”€â”€â”˜
                                                    
                                                    = 1 seule lecture !
```

### cache() vs persist()

```python
# cache() = persist() avec MEMORY_ONLY
df.cache()

# persist() = plus d'options
from pyspark import StorageLevel

df.persist(StorageLevel.MEMORY_ONLY)          # RAM seulement (= cache)
df.persist(StorageLevel.DISK_ONLY)            # Disque seulement  
df.persist(StorageLevel.MEMORY_AND_DISK)      # RAM, dÃ©borde sur disque
df.persist(StorageLevel.MEMORY_ONLY_SER)      # RAM, sÃ©rialisÃ© (compact)
df.persist(StorageLevel.MEMORY_AND_DISK_SER)  # RAM sÃ©rialisÃ© + disque
```

### Niveaux de stockage

| Niveau | RAM | Disque | SÃ©rialisÃ© | Usage |
|--------|-----|--------|-----------|-------|
| `MEMORY_ONLY` | âœ… | âŒ | âŒ | DÃ©faut, plus rapide |
| `MEMORY_AND_DISK` | âœ… | âœ… | âŒ | Si RAM insuffisante |
| `DISK_ONLY` | âŒ | âœ… | âŒ | TrÃ¨s grandes donnÃ©es |
| `MEMORY_ONLY_SER` | âœ… | âŒ | âœ… | Ã‰conomise RAM |
| `MEMORY_AND_DISK_SER` | âœ… | âœ… | âœ… | Ã‰conomise RAM + dÃ©borde |

### Quand utiliser ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QUAND UTILISER CACHE ?                                  â”‚
â”‚                                                                          â”‚
â”‚  âœ… UTILISER cache() si:                                                â”‚
â”‚     - DataFrame rÃ©utilisÃ© plusieurs fois                                 â”‚
â”‚     - Calcul coÃ»teux (joins, aggregations)                               â”‚
â”‚     - DonnÃ©es tiennent en mÃ©moire                                        â”‚
â”‚     - Machine Learning (itÃ©rations)                                      â”‚
â”‚                                                                          â”‚
â”‚  âŒ NE PAS utiliser cache() si:                                         â”‚
â”‚     - DataFrame utilisÃ© une seule fois                                   â”‚
â”‚     - DonnÃ©es trop grandes pour la RAM                                   â”‚
â”‚     - OpÃ©rations simples (filter, select)                                â”‚
â”‚                                                                          â”‚
â”‚  âš ï¸ IMPORTANT: LibÃ©rer le cache quand plus besoin                       â”‚
â”‚     df.unpersist()                                                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemple complet

```python
# Lecture
df = spark.read.csv("users.csv")

# Transformation coÃ»teuse
df_processed = df.filter(df.age > 18) \
                 .join(other_df, "user_id") \
                 .groupBy("city") \
                 .agg({"salary": "avg"})

# Cache car rÃ©utilisÃ©
df_processed.cache()

# PremiÃ¨re action â†’ calcule ET cache
df_processed.count()

# Utilisation 2 â†’ lit du cache (rapide)
df_processed.filter(df_processed.city == "Tunis").show()

# Utilisation 3 â†’ lit du cache (rapide)
df_processed.write.parquet("output")

# LibÃ©rer la mÃ©moire
df_processed.unpersist()
```

---

## 11. âš ï¸ Data Skew

### C'est quoi ?

Quand une partition a **beaucoup plus de donnÃ©es** que les autres :

```
DONNÃ‰ES Ã‰QUILIBRÃ‰ES (OK):           DONNÃ‰ES SKEW (PROBLÃˆME):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task 0   â”‚ â–ˆâ–ˆâ–ˆâ–ˆ 2min              â”‚ Task 0   â”‚ â–ˆâ–ˆ 1min
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Task 1   â”‚ â–ˆâ–ˆâ–ˆâ–ˆ 2min              â”‚ Task 1   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30min âš ï¸
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Task 2   â”‚ â–ˆâ–ˆâ–ˆâ–ˆ 2min              â”‚ Task 2   â”‚ â–ˆâ–ˆ 1min
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: 2min                         Total: 30min (1 task bloque tout!)
```

### Causes courantes

```python
# Exemple: groupBy sur une clÃ© dÃ©sÃ©quilibrÃ©e
df.groupBy("country").count()

# Distribution des donnÃ©es:
# USA: 10,000,000 lignes  â† 90% des donnÃ©es!
# France: 500,000 lignes
# Tunisia: 500,000 lignes

# â†’ La partition "USA" prend 10x plus de temps
```

### Comment dÃ©tecter ?

```
Dans Spark UI â†’ Stage â†’ Tasks:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task ID â”‚ Duration â”‚ Shuffle Read â”‚ Status                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    0    â”‚    2s    â”‚    10 MB     â”‚ SUCCESS                          â”‚
â”‚    1    â”‚   45min  â”‚   900 MB     â”‚ RUNNING â† SKEW!                  â”‚
â”‚    2    â”‚    3s    â”‚    15 MB     â”‚ SUCCESS                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Si 1 task dure 10x+ plus que les autres = SKEW
```

### Solutions

| Solution | Description | Exemple |
|----------|-------------|---------|
| **Salting** | Ajouter clÃ© alÃ©atoire pour distribuer | `df.withColumn("salt", rand() % 10)` |
| **Broadcast Join** | Si une table est petite | `broadcast(small_df)` |
| **Adaptive Query Execution** | Spark 3+ auto-optimise | `spark.sql.adaptive.enabled=true` |
| **Repartition** | Redistribuer avant opÃ©ration | `df.repartition(100, "key")` |

### Exemple de Salting

```python
# AVANT (skew sur country):
df.groupBy("country").count()

# APRÃˆS (avec salting):
from pyspark.sql.functions import rand, concat, lit

# Ajouter un "salt" alÃ©atoire
df_salted = df.withColumn("salted_key", 
    concat(df.country, lit("_"), (rand() * 10).cast("int")))

# GroupBy sur la clÃ© saltÃ©e
result = df_salted.groupBy("salted_key").count()

# ReagrÃ©ger par country original
final = result.withColumn("country", 
    split(col("salted_key"), "_")[0]) \
    .groupBy("country").sum("count")
```

---

## 12. ğŸ“¡ Broadcast Variables

### Le problÃ¨me

```
SANS BROADCAST:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Driver  â”‚         â”‚  Executor 1 â”‚  â† ReÃ§oit petite_table (100MB)
â”‚         â”‚ â”€â”€â”€â”€â”€â”€â–º â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ petite  â”‚         â”‚  Executor 2 â”‚  â† ReÃ§oit petite_table (100MB)
â”‚ _table  â”‚ â”€â”€â”€â”€â”€â”€â–º â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ (100MB) â”‚         â”‚  Executor 3 â”‚  â† ReÃ§oit petite_table (100MB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”€â”€â”€â”€â”€â”€â–º â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

= 100MB Ã— 3 = 300MB transfÃ©rÃ©s !
(et pire: envoyÃ© pour CHAQUE task!)
```

### La solution : Broadcast

```python
from pyspark.sql.functions import broadcast

# Broadcast la petite table
result = big_df.join(broadcast(small_df), "id")
```

```
AVEC BROADCAST:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Driver  â”‚         â”‚  Executor 1 â”‚  
â”‚         â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚  (cache     â”‚  â† 1 seule copie partagÃ©e par toutes les tasks
â”‚ petite  â”‚   1x    â”‚   local)    â”‚
â”‚ _table  â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ (100MB) â”‚         â”‚  Executor 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

= 100MB total seulement !
```

### Quand utiliser ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BROADCAST JOIN                                          â”‚
â”‚                                                                          â”‚
â”‚  âœ… UTILISER si:                                                        â”‚
â”‚     - Une table < 10 MB (dÃ©faut) ou jusqu'Ã  ~1 GB (configurable)        â”‚
â”‚     - Join frÃ©quent avec la mÃªme petite table                            â”‚
â”‚     - Table de rÃ©fÃ©rence (pays, devises, catÃ©gories...)                 â”‚
â”‚                                                                          â”‚
â”‚  âŒ NE PAS utiliser si:                                                 â”‚
â”‚     - Deux grandes tables                                                â”‚
â”‚     - Table trop grande pour la mÃ©moire des executors                   â”‚
â”‚                                                                          â”‚
â”‚  Config:                                                                 â”‚
â”‚  spark.sql.autoBroadcastJoinThreshold = 10485760 (10MB par dÃ©faut)      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Types de joins

| Type | Quand utiliser |
|------|----------------|
| **Broadcast Hash Join** | Petite table (< 10MB) |
| **Sort Merge Join** | Deux grandes tables, triÃ©es |
| **Shuffle Hash Join** | Tables moyennes |

---

## 13. ğŸ“ Formats de fichiers

### Comparaison

| Format | Compression | Lecture colonnes | SchÃ©ma | Usage |
|--------|-------------|------------------|--------|-------|
| **Parquet** | âœ… Excellente | âœ… Oui | âœ… IntÃ©grÃ© | Standard Big Data |
| **ORC** | âœ… Excellente | âœ… Oui | âœ… IntÃ©grÃ© | Hive ecosystem |
| **Avro** | âœ… Bonne | âŒ Non | âœ… IntÃ©grÃ© | Streaming, Kafka |
| **CSV** | âŒ Non | âŒ Non | âŒ Non | Import/Export simple |
| **JSON** | âŒ Non | âŒ Non | âŒ Non | APIs, logs |

### Pourquoi Parquet est recommandÃ© ?

```
CSV: Lit TOUT le fichier
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id â”‚ name â”‚ age â”‚ city â”‚ salary â”‚ ... â”‚  â† Lit toutes les colonnes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Parquet: Lit SEULEMENT les colonnes demandÃ©es
â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ name â”‚  â† Lit uniquement "name"
â””â”€â”€â”€â”€â”€â”€â”˜

= Beaucoup plus rapide et moins de I/O !
```

```python
# CSV: lit tout mÃªme si on veut 1 colonne
df = spark.read.csv("data.csv")
df.select("name").show()  # Lit toutes les colonnes quand mÃªme!

# PARQUET: lit seulement ce qui est demandÃ©
df = spark.read.parquet("data.parquet")
df.select("name").show()  # Lit uniquement la colonne "name"
```

### Ã‰crire en Parquet

```python
# Ã‰crire
df.write.parquet("output/data.parquet")

# Avec partitionnement (trÃ¨s important pour les grosses tables)
df.write.partitionBy("year", "month").parquet("output/data.parquet")

# RÃ©sultat:
# output/data.parquet/
#   year=2024/
#     month=01/
#       part-00000.parquet
#     month=02/
#       part-00000.parquet
#   year=2025/
#     ...
```

---

## 14. ğŸ–¥ï¸ Spark UI

### AccÃ¨s

```
Pendant l'exÃ©cution:  http://localhost:4040
History Server:       http://localhost:18080
```

### Onglets importants

| Onglet | Ce qu'il montre | UtilitÃ© |
|--------|----------------|---------|
| **Jobs** | Liste des jobs | Vue globale, durÃ©e |
| **Stages** | DÃ©tail des stages | Identifier les shuffles |
| **Tasks** | DÃ©tail des tasks | DÃ©tecter le skew |
| **Storage** | DataFrames cachÃ©s | VÃ©rifier le cache |
| **Executors** | Ã‰tat des executors | MÃ©moire, GC, cores |
| **SQL** | Plans d'exÃ©cution | Optimisation requÃªtes |
| **Environment** | Configuration | VÃ©rifier les settings |

### Que regarder pour diagnostiquer ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DIAGNOSTIC SPARK UI                                   â”‚
â”‚                                                                          â”‚
â”‚  JOB LENT ?                                                             â”‚
â”‚  â†’ Jobs tab â†’ identifier quel stage est lent                            â”‚
â”‚                                                                          â”‚
â”‚  STAGE LENT ?                                                           â”‚
â”‚  â†’ Stages tab â†’ regarder Shuffle Read/Write                             â”‚
â”‚  â†’ Si shuffle Ã©levÃ© â†’ trop de donnÃ©es redistribuÃ©es                     â”‚
â”‚                                                                          â”‚
â”‚  DATA SKEW ?                                                            â”‚
â”‚  â†’ Stage â†’ Tasks â†’ comparer durÃ©es                                      â”‚
â”‚  â†’ Si 1 task >> autres â†’ SKEW                                           â”‚
â”‚                                                                          â”‚
â”‚  MÃ‰MOIRE ?                                                              â”‚
â”‚  â†’ Executors tab â†’ GC Time                                              â”‚
â”‚  â†’ Si GC Time > 10% â†’ problÃ¨me mÃ©moire                                  â”‚
â”‚                                                                          â”‚
â”‚  CACHE OK ?                                                             â”‚
â”‚  â†’ Storage tab â†’ voir si DataFrame cachÃ©                                â”‚
â”‚  â†’ Fraction Cached devrait Ãªtre 100%                                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemple Stages tab

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK UI - Stages Tab                               â”‚
â”‚                                                                          â”‚
â”‚ Stage â”‚ Description       â”‚ Tasks â”‚ Input  â”‚ Shuffle Write â”‚ Duration  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚   0   â”‚ csv + filter      â”‚ 10/10 â”‚ 500 MB â”‚ 100 MB        â”‚   15s     â”‚
â”‚   1   â”‚ groupBy           â”‚  5/5  â”‚ 100 MB â”‚ 20 MB         â”‚   45s âš ï¸  â”‚
â”‚   2   â”‚ orderBy           â”‚  1/1  â”‚ 20 MB  â”‚ -             â”‚   5s      â”‚
â”‚                                                                          â”‚
â”‚ â†’ Stage 1 est lent, vÃ©rifier les tasks pour skew                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 15. âš™ï¸ Configurations

### Configurations importantes

| Config | Description | Recommandation |
|--------|-------------|----------------|
| `spark.executor.memory` | RAM par executor | 4-8 GB typique |
| `spark.executor.cores` | Cores par executor | 4-5 cores max |
| `spark.driver.memory` | RAM du driver | 2-4 GB |
| `spark.executor.instances` | Nombre d'executors | Selon cluster |
| `spark.sql.shuffle.partitions` | Partitions aprÃ¨s shuffle | 200 par dÃ©faut, ajuster |
| `spark.default.parallelism` | ParallÃ©lisme par dÃ©faut | 2-4 Ã— total cores |

### Formule de configuration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RÃˆGLES DE CONFIGURATION                                 â”‚
â”‚                                                                          â”‚
â”‚  CORES PAR EXECUTOR: 4-5                                                â”‚
â”‚  â†’ Plus de 5 = problÃ¨mes de GC et HDFS throughput                       â”‚
â”‚                                                                          â”‚
â”‚  MÃ‰MOIRE PAR EXECUTOR:                                                  â”‚
â”‚  â†’ (RAM node - overhead OS) / nb executors par node                     â”‚
â”‚  â†’ Garder ~10% pour overhead (spark.executor.memoryOverhead)            â”‚
â”‚                                                                          â”‚
â”‚  NOMBRE DE PARTITIONS:                                                  â”‚
â”‚  â†’ Pour shuffle: 2-3 Ã— total_cores                                      â”‚
â”‚  â†’ Taille partition idÃ©ale: 128 MB - 1 GB                               â”‚
â”‚                                                                          â”‚
â”‚  EXEMPLE:                                                                â”‚
â”‚  Cluster: 10 nodes Ã— 16 cores Ã— 64 GB RAM                               â”‚
â”‚  â†’ executors: 10 Ã— 3 = 30 executors (3 par node)                        â”‚
â”‚  â†’ cores: 5 par executor                                                 â”‚
â”‚  â†’ memory: ~18 GB par executor                                          â”‚
â”‚  â†’ partitions: 30 Ã— 5 Ã— 2 = 300 partitions                              â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configurations Spark SQL

```python
# Augmenter partitions aprÃ¨s shuffle
spark.conf.set("spark.sql.shuffle.partitions", 500)

# Activer Adaptive Query Execution (Spark 3+)
spark.conf.set("spark.sql.adaptive.enabled", "true")

# Seuil pour broadcast join
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 100*1024*1024)  # 100MB
```

---

## 16. ğŸ”§ Commandes spark-submit

### Syntaxe de base

```bash
spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  --executor-memory <memory> \
  --executor-cores <cores> \
  --num-executors <num> \
  <application-jar-or-python-file> \
  [application-arguments]
```

### Exemples courants

```bash
# Mode local (test/dÃ©veloppement)
spark-submit --master local[4] mon_script.py

# Mode local avec toute la RAM
spark-submit --master local[*] \
  --driver-memory 4g \
  mon_script.py

# Mode YARN cluster (production)
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 4 \
  --driver-memory 2g \
  mon_script.py

# Avec configurations supplÃ©mentaires
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.sql.shuffle.partitions=500 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.executor.memoryOverhead=1g \
  mon_script.py

# Avec dÃ©pendances
spark-submit \
  --master yarn \
  --jars extra-library.jar \
  --py-files utils.py,helpers.py \
  --packages org.apache.spark:spark-sql-kafka:3.0.0 \
  mon_script.py
```

### Options importantes

| Option | Description |
|--------|-------------|
| `--master` | URL du cluster (local, yarn, mesos, k8s) |
| `--deploy-mode` | `client` (driver local) ou `cluster` (driver sur cluster) |
| `--executor-memory` | RAM par executor |
| `--executor-cores` | Cores par executor |
| `--num-executors` | Nombre d'executors |
| `--driver-memory` | RAM du driver |
| `--conf` | Configuration Spark |
| `--jars` | JARs supplÃ©mentaires |
| `--py-files` | Fichiers Python supplÃ©mentaires |
| `--packages` | DÃ©pendances Maven |

### Modes de dÃ©ploiement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  CLIENT MODE:                     CLUSTER MODE:                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   TON PC       â”‚              â”‚    CLUSTER     â”‚                     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                     â”‚
â”‚  â”‚  â”‚  DRIVER  â”‚  â”‚              â”‚  â”‚  DRIVER  â”‚  â”‚                     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                     â”‚
â”‚          â”‚                       â”‚  â”‚ EXECUTOR â”‚  â”‚                     â”‚
â”‚          â–¼                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  â”‚    CLUSTER     â”‚                                                      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              UtilisÃ© en PRODUCTION                  â”‚
â”‚  â”‚  â”‚ EXECUTOR â”‚  â”‚              Driver sur le cluster                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â”‚                                                                          â”‚
â”‚  UtilisÃ© pour DEBUG                                                      â”‚
â”‚  Driver sur ta machine                                                   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 17. âŒ Erreurs courantes

| Erreur | Cause | Solution |
|--------|-------|----------|
| `OutOfMemoryError: Java heap space` | Executor manque de RAM | Augmenter `executor-memory` |
| `OutOfMemoryError: GC overhead` | Trop d'objets en mÃ©moire | RÃ©duire `executor-cores`, augmenter RAM |
| `Container killed by YARN` | DÃ©passement mÃ©moire | Augmenter `spark.executor.memoryOverhead` |
| `Task not serializable` | Objet non sÃ©rialisable | Utiliser broadcast ou rÃ©Ã©crire le code |
| `Shuffle fetch failed` | Executor crash pendant shuffle | VÃ©rifier logs, augmenter timeout |
| `Job aborted: Stage failure` | Task Ã©choue X fois | VÃ©rifier donnÃ©es et code |
| `File not found` | Chemin incorrect | VÃ©rifier chemin HDFS/S3 |
| `AnalysisException` | Erreur SQL/DataFrame | VÃ©rifier noms colonnes, schÃ©ma |

### Diagnostic rapide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DIAGNOSTIC RAPIDE                                     â”‚
â”‚                                                                          â”‚
â”‚  1. OOM Error ?                                                          â”‚
â”‚     â†’ Spark UI â†’ Executors â†’ Memory usage                               â”‚
â”‚     â†’ Augmenter executor-memory                                         â”‚
â”‚     â†’ Repartitionner les donnÃ©es                                        â”‚
â”‚                                                                          â”‚
â”‚  2. Job trÃ¨s lent ?                                                      â”‚
â”‚     â†’ Spark UI â†’ Stages â†’ identifier stage lent                         â”‚
â”‚     â†’ VÃ©rifier shuffle read/write                                       â”‚
â”‚     â†’ VÃ©rifier skew dans Tasks                                          â”‚
â”‚                                                                          â”‚
â”‚  3. GC Time Ã©levÃ© ?                                                      â”‚
â”‚     â†’ Spark UI â†’ Executors â†’ GC Time                                    â”‚
â”‚     â†’ Si > 10% â†’ rÃ©duire cores, augmenter RAM                           â”‚
â”‚     â†’ Utiliser Kryo serialization                                       â”‚
â”‚                                                                          â”‚
â”‚  4. Shuffle trop gros ?                                                  â”‚
â”‚     â†’ RÃ©duire donnÃ©es avant shuffle (filter tÃ´t)                        â”‚
â”‚     â†’ Utiliser broadcast join                                           â”‚
â”‚     â†’ Augmenter spark.sql.shuffle.partitions                            â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 18. ğŸ“‹ Checklist Entretien Junior

### Ce que tu dois savoir expliquer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                SPARK - CE QUE TU DOIS SAVOIR                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚ POURQUOI SPARK:                                                         â”‚
â”‚ â–¡ Plus rapide que MapReduce (RAM vs Disque)                             â”‚
â”‚ â–¡ API simple, supporte batch + streaming + ML                           â”‚
â”‚                                                                          â”‚
â”‚ ARCHITECTURE:                                                           â”‚
â”‚ â–¡ Driver = programme principal, planifie                                â”‚
â”‚ â–¡ Executor = processus JVM qui exÃ©cute les tasks                        â”‚
â”‚ â–¡ Worker Node = machine qui hÃ©berge executors                           â”‚
â”‚ â–¡ Cluster Manager = YARN, Mesos, K8s                                    â”‚
â”‚                                                                          â”‚
â”‚ CONCEPTS CLÃ‰S:                                                          â”‚
â”‚ â–¡ Lazy Evaluation = transformations pas exÃ©cutÃ©es seules                â”‚
â”‚ â–¡ DAG = plan d'exÃ©cution optimisÃ©                                       â”‚
â”‚ â–¡ Transformation vs Action                                              â”‚
â”‚ â–¡ Narrow vs Wide transformations                                        â”‚
â”‚ â–¡ Job â†’ Stage â†’ Task (hiÃ©rarchie)                                       â”‚
â”‚ â–¡ 1 Action = 1 Job                                                      â”‚
â”‚ â–¡ 1 Shuffle = nouvelle Stage                                            â”‚
â”‚ â–¡ 1 Partition = 1 Task                                                  â”‚
â”‚                                                                          â”‚
â”‚ SHUFFLE:                                                                â”‚
â”‚ â–¡ C'est quoi = redistribution donnÃ©es via rÃ©seau                        â”‚
â”‚ â–¡ Pourquoi lent = I/O disque + rÃ©seau + sÃ©rialisation                   â”‚
â”‚ â–¡ OpÃ©rations = groupBy, join, orderBy, distinct                         â”‚
â”‚                                                                          â”‚
â”‚ PARTITIONS:                                                             â”‚
â”‚ â–¡ Morceau de donnÃ©es                                                    â”‚
â”‚ â–¡ Taille idÃ©ale = 128MB - 1GB                                           â”‚
â”‚ â–¡ repartition vs coalesce                                               â”‚
â”‚                                                                          â”‚
â”‚ CACHE:                                                                  â”‚
â”‚ â–¡ Garder en mÃ©moire pour rÃ©utilisation                                  â”‚
â”‚ â–¡ cache() vs persist()                                                  â”‚
â”‚ â–¡ Quand utiliser = DataFrame rÃ©utilisÃ© plusieurs fois                   â”‚
â”‚                                                                          â”‚
â”‚ OPTIMISATION:                                                           â”‚
â”‚ â–¡ Data Skew = dÃ©sÃ©quilibre des partitions                               â”‚
â”‚ â–¡ Broadcast Join = pour petites tables                                   â”‚
â”‚ â–¡ Parquet = format recommandÃ©                                           â”‚
â”‚                                                                          â”‚
â”‚ DEBUGGING:                                                              â”‚
â”‚ â–¡ Spark UI = Jobs, Stages, Tasks                                        â”‚
â”‚ â–¡ Identifier les shuffles (Shuffle Read/Write)                          â”‚
â”‚ â–¡ DÃ©tecter le skew (tasks longues)                                      â”‚
â”‚ â–¡ VÃ©rifier GC Time                                                      â”‚
â”‚                                                                          â”‚
â”‚ COMMANDES:                                                              â”‚
â”‚ â–¡ spark-submit avec options principales                                 â”‚
â”‚ â–¡ --master, --executor-memory, --num-executors                          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Questions d'entretien types

| Question | Points clÃ©s Ã  mentionner |
|----------|-------------------------|
| C'est quoi le shuffle et pourquoi Ã§a ralentit Spark ? | Redistribution rÃ©seau, I/O disque, causÃ© par groupBy/join/orderBy |
| DiffÃ©rence transformation vs action ? | Lazy vs dÃ©clenche exÃ©cution, exemples de chaque |
| Pourquoi un job Spark peut Ãªtre lent ? | Skew, trop de shuffles, mauvaise config mÃ©moire, petits fichiers |
| cache() vs persist() ? | cache = MEMORY_ONLY, persist = plus d'options de stockage |
| Comment optimiser un join ? | Broadcast si petite table, partitionner sur clÃ© de join |
| C'est quoi le DAG ? | Plan d'exÃ©cution, optimisÃ© par Catalyst avant exÃ©cution |
| Job vs Stage vs Task ? | Actionâ†’Job, Shuffleâ†’Stage, Partitionâ†’Task |
| repartition vs coalesce ? | repartition=shuffle, coalesce=pas shuffle (diminuer seulement) |

---

## 19. ğŸ¯ RÃ©sumÃ© en une page

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           SPARK EN BREF                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                            â•‘
â•‘  POURQUOI:      Plus rapide que MapReduce (RAM vs Disque)                 â•‘
â•‘                                                                            â•‘
â•‘  ARCHITECTURE:  Driver â†’ Cluster Manager â†’ Workers â†’ Executors â†’ Tasks    â•‘
â•‘                                                                            â•‘
â•‘  LAZY EVAL:     Transformations construisent le DAG, Actions exÃ©cutent    â•‘
â•‘                                                                            â•‘
â•‘  HIÃ‰RARCHIE:    1 Action = 1 Job | 1 Shuffle = 1 Stage | 1 Partition = 1 Task â•‘
â•‘                                                                            â•‘
â•‘  SHUFFLE:       Redistribution rÃ©seau (groupBy, join, orderBy) = LENT    â•‘
â•‘                                                                            â•‘
â•‘  NARROW:        filter, select, map â†’ pas de shuffle â†’ RAPIDE            â•‘
â•‘                                                                            â•‘
â•‘  WIDE:          groupBy, join, orderBy â†’ shuffle â†’ LENT                  â•‘
â•‘                                                                            â•‘
â•‘  PARTITIONS:    Taille idÃ©ale 128MB-1GB | repartition vs coalesce        â•‘
â•‘                                                                            â•‘
â•‘  CACHE:         Garder en RAM si rÃ©utilisÃ© | unpersist() pour libÃ©rer    â•‘
â•‘                                                                            â•‘
â•‘  SKEW:          1 partition >> autres â†’ vÃ©rifier dans Spark UI           â•‘
â•‘                                                                            â•‘
â•‘  BROADCAST:     Petite table < 10MB â†’ Ã©vite shuffle du join              â•‘
â•‘                                                                            â•‘
â•‘  FORMAT:        Parquet recommandÃ© (colonnes, compression)                â•‘
â•‘                                                                            â•‘
â•‘  DIAGNOSTIC:    Spark UI â†’ Stages â†’ Tasks â†’ durÃ©e, shuffle, GC           â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

> **Bonne chance pour ton entretien !** ğŸš€