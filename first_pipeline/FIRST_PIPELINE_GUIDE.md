# ğŸš€ Premier Pipeline Big Data - Guide Complet

> **Objectif** : Apprendre Ã  crÃ©er, configurer et dÃ©ployer ton premier pipeline Big Data avec Docker

---

## Table des matiÃ¨res

1. [C'est quoi un Pipeline Big Data ?](#1--cest-quoi-un-pipeline-big-data)
2. [Architecture d'un Pipeline](#2-ï¸-architecture-dun-pipeline)
3. [PrÃ©requis et Installation](#3--prÃ©requis-et-installation)
4. [Docker pour le Big Data](#4--docker-pour-le-big-data)
5. [CrÃ©er ton Premier Pipeline](#5--crÃ©er-ton-premier-pipeline)
6. [Configuration de chaque Composant](#6-ï¸-configuration-de-chaque-composant)
7. [Docker Compose - Le Chef d'Orchestre](#7--docker-compose---le-chef-dorchestre)
8. [DÃ©marrer et Tester](#8--dÃ©marrer-et-tester)
9. [Debugging et Troubleshooting](#9--debugging-et-troubleshooting)
10. [Bonnes Pratiques](#10--bonnes-pratiques)
11. [Checklist de DÃ©marrage](#11--checklist-de-dÃ©marrage)

---

## 1. ğŸ“Š C'est quoi un Pipeline Big Data ?

### DÃ©finition Simple

Un **pipeline Big Data** c'est comme une **chaÃ®ne de production** pour les donnÃ©es :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PIPELINE = CHAÃNE DE DONNÃ‰ES                        â”‚
â”‚                                                                          â”‚
â”‚   SOURCE        INGESTION       TRAITEMENT       STOCKAGE       OUTPUT  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ API  â”‚  â”€â”€â–º  â”‚Kafka â”‚  â”€â”€â–º   â”‚Spark â”‚  â”€â”€â–º   â”‚  DB  â”‚  â”€â”€â–º  â”‚Grafanaâ”‚â”‚
â”‚  â”‚ File â”‚       â”‚      â”‚        â”‚      â”‚        â”‚      â”‚       â”‚       â”‚â”‚
â”‚  â”‚ DB   â”‚       â”‚      â”‚        â”‚      â”‚        â”‚      â”‚       â”‚       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                          â”‚
â”‚  DonnÃ©es       Messages        Calculs         Sauvegarde    Visualiser â”‚
â”‚  brutes        en temps        distribuÃ©s      structurÃ©e    et alerter â”‚
â”‚                rÃ©el                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les 5 Ã‰tapes d'un Pipeline

| Ã‰tape | RÃ´le | Outils courants |
|-------|------|-----------------|
| **1. Source** | D'oÃ¹ viennent les donnÃ©es | APIs, fichiers, bases de donnÃ©es, IoT |
| **2. Ingestion** | Collecter et transporter | Kafka, RabbitMQ, Kinesis |
| **3. Traitement** | Transformer et calculer | Spark, Flink, Storm |
| **4. Stockage** | Sauvegarder les rÃ©sultats | PostgreSQL, Cassandra, HDFS, S3 |
| **5. Consommation** | Utiliser les donnÃ©es | Grafana, Tableau, APIs |

### Batch vs Streaming

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  BATCH (par lots)                    STREAMING (temps rÃ©el)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ DonnÃ©es du jour   â”‚              â”‚ DonnÃ©es continues â”‚               â”‚
â”‚  â”‚ accumulÃ©es        â”‚              â”‚ flux constant     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚            â”‚                                  â”‚                          â”‚
â”‚            â–¼ (1 fois/jour)                    â–¼ (continu)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Traitement      â”‚              â”‚   Traitement      â”‚               â”‚
â”‚  â”‚   (ex: 2h)        â”‚              â”‚   (ex: < 1s)      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                          â”‚
â”‚  Exemples:                          Exemples:                           â”‚
â”‚  - Rapports quotidiens              - Alertes en temps rÃ©el             â”‚
â”‚  - Analyses historiques             - Dashboards live                   â”‚
â”‚  - ML training                      - DÃ©tection de fraude               â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ğŸ—ï¸ Architecture d'un Pipeline

### Architecture Typique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE PIPELINE BIG DATA                        â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    COUCHE ORCHESTRATION                          â”‚    â”‚
â”‚  â”‚                        (Airflow)                                 â”‚    â”‚
â”‚  â”‚         Planifie, dÃ©clenche et monitore les jobs                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                               â”‚                                          â”‚
â”‚                               â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    COUCHE INGESTION                              â”‚    â”‚
â”‚  â”‚                        (Kafka)                                   â”‚    â”‚
â”‚  â”‚              Buffer et transport des messages                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                               â”‚                                          â”‚
â”‚                               â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    COUCHE TRAITEMENT                             â”‚    â”‚
â”‚  â”‚                        (Spark)                                   â”‚    â”‚
â”‚  â”‚              Transformation et calculs distribuÃ©s               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                               â”‚                                          â”‚
â”‚                               â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    COUCHE STOCKAGE                               â”‚    â”‚
â”‚  â”‚              PostgreSQL (SQL) / Cassandra (NoSQL)               â”‚    â”‚
â”‚  â”‚                   Persistance des donnÃ©es                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                               â”‚                                          â”‚
â”‚                               â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    COUCHE VISUALISATION                          â”‚    â”‚
â”‚  â”‚                        (Grafana)                                 â”‚    â”‚
â”‚  â”‚                   Dashboards et alertes                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack Technique RecommandÃ©e pour DÃ©butants

| Composant | Technologie | Pourquoi ? |
|-----------|-------------|------------|
| **Orchestration** | Apache Airflow | Interface web, facile Ã  apprendre |
| **Message Queue** | Apache Kafka | Standard industrie, trÃ¨s documentÃ© |
| **Processing** | Apache Spark | Puissant, supporte batch + streaming |
| **SQL Database** | PostgreSQL | Robuste, ACID, gratuit |
| **NoSQL Database** | Cassandra | Scalable, rapide en Ã©criture |
| **Monitoring** | Grafana + Prometheus | Dashboards, mÃ©triques, alertes |
| **Container** | Docker + Compose | DÃ©ploiement simple et reproductible |

---

## 3. ğŸ“¦ PrÃ©requis et Installation

### MatÃ©riel Minimum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONFIGURATION MINIMALE                                â”‚
â”‚                                                                          â”‚
â”‚  RAM:        8 GB minimum (16 GB recommandÃ©)                            â”‚
â”‚  CPU:        4 cores minimum                                            â”‚
â”‚  Disque:     20 GB d'espace libre                                       â”‚
â”‚  OS:         Windows 10/11, macOS 10.15+, Linux Ubuntu 20.04+          â”‚
â”‚                                                                          â”‚
â”‚  âš ï¸  Si RAM < 8GB: RÃ©duire le nombre de services                       â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation Docker

#### Windows
```bash
# 1. TÃ©lÃ©charger Docker Desktop
# https://www.docker.com/products/docker-desktop/

# 2. Installer et redÃ©marrer

# 3. VÃ©rifier l'installation
docker --version
docker-compose --version
```

#### macOS
```bash
# Option 1: TÃ©lÃ©charger Docker Desktop
# https://www.docker.com/products/docker-desktop/

# Option 2: Homebrew
brew install --cask docker

# VÃ©rifier
docker --version
docker-compose --version
```

#### Linux (Ubuntu/Debian)
```bash
# Mettre Ã  jour
sudo apt update

# Installer les dÃ©pendances
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common

# Ajouter la clÃ© GPG Docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Ajouter le repository
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Installer Docker
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# Ajouter ton user au groupe docker
sudo usermod -aG docker $USER

# DÃ©connecter/reconnecter puis vÃ©rifier
docker --version
docker compose version
```

### VÃ©rification de l'Installation

```bash
# Test Docker
docker run hello-world

# Test Docker Compose
docker compose version

# VÃ©rifier les ressources
docker system info | grep -E "Memory|CPUs"
```

---

## 4. ğŸ³ Docker pour le Big Data

### Pourquoi Docker ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AVANTAGES DE DOCKER                                   â”‚
â”‚                                                                          â”‚
â”‚  SANS DOCKER:                          AVEC DOCKER:                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚                                                                          â”‚
â”‚  "Ã‡a marche sur ma machine"            "Ã‡a marche PARTOUT"              â”‚
â”‚                                                                          â”‚
â”‚  - Installer Java 8, 11, 17...         - 1 commande: docker-compose up  â”‚
â”‚  - Configurer variables env            - Tout prÃ©configurÃ©              â”‚
â”‚  - RÃ©soudre conflits de versions       - Environnement isolÃ©            â”‚
â”‚  - Docs diffÃ©rentes par OS             - MÃªme comportement partout      â”‚
â”‚  - 2h d'installation                   - 5 minutes                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Concepts Docker Essentiels

| Concept | DÃ©finition | Analogie |
|---------|------------|----------|
| **Image** | Template en lecture seule | Recette de cuisine |
| **Container** | Instance d'une image en cours d'exÃ©cution | Plat prÃ©parÃ© |
| **Volume** | Stockage persistant | Frigo (donnÃ©es gardÃ©es) |
| **Network** | RÃ©seau virtuel entre containers | CÃ¢bles rÃ©seau |
| **Docker Compose** | DÃ©finit plusieurs services | Menu complet |

### Structure d'un Projet Docker

```
mon-pipeline/
â”œâ”€â”€ docker-compose.yml          # â† DÃ©finit TOUS les services
â”œâ”€â”€ .env                        # â† Variables d'environnement
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ airflow/               # Config Airflow
â”‚   â”œâ”€â”€ kafka/                 # Config Kafka
â”‚   â”œâ”€â”€ spark/                 # Config Spark
â”‚   â”œâ”€â”€ prometheus.yml         # Config monitoring
â”‚   â””â”€â”€ grafana/               # Dashboards Grafana
â”œâ”€â”€ dags/                      # â† DAGs Airflow (jobs)
â”œâ”€â”€ spark/                     # â† Scripts Spark
â”œâ”€â”€ scripts/                   # â† Scripts utilitaires
â”œâ”€â”€ data/                      # â† DonnÃ©es (si besoin)
â””â”€â”€ README.md                  # â† Documentation
```

### Commandes Docker Essentielles

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    GESTION DES CONTAINERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# DÃ©marrer tous les services
docker-compose up -d

# Voir les services en cours
docker-compose ps

# Voir les logs (tous les services)
docker-compose logs -f

# Voir les logs d'un service
docker-compose logs -f kafka

# ArrÃªter tous les services
docker-compose down

# ArrÃªter et supprimer les volumes (RESET COMPLET)
docker-compose down -v


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    INTERACTION AVEC CONTAINERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Entrer dans un container
docker exec -it kafka bash
docker exec -it spark-master bash
docker exec -it postgres psql -U airflow -d mydb

# ExÃ©cuter une commande
docker exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# Copier un fichier vers un container
docker cp local_file.txt container_name:/path/in/container/

# Copier un fichier depuis un container
docker cp container_name:/path/in/container/file.txt ./local/


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    MONITORING ET DEBUG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Voir l'utilisation des ressources
docker stats

# Inspecter un container
docker inspect container_name

# Voir les rÃ©seaux
docker network ls

# Voir les volumes
docker volume ls

# Nettoyer les ressources inutilisÃ©es
docker system prune -a
```

---

## 5. ğŸ”§ CrÃ©er ton Premier Pipeline

### Objectif du Pipeline

On va crÃ©er un pipeline simple qui :
1. **Collecte** des donnÃ©es depuis une API (ou gÃ©nÃ¨re des donnÃ©es)
2. **IngÃ¨re** dans Kafka
3. **Traite** avec Spark
4. **Stocke** dans PostgreSQL
5. **Visualise** avec Grafana

### Structure du Projet

```bash
# CrÃ©er la structure
mkdir -p mon-premier-pipeline/{config,dags,spark,scripts,data}
cd mon-premier-pipeline

# CrÃ©er les fichiers
touch docker-compose.yml
touch .env
touch config/prometheus.yml
touch dags/mon_premier_dag.py
touch spark/mon_premier_job.py
touch scripts/test_connexion.py
```

### Fichier .env (Variables d'Environnement)

```bash
# .env

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    VERSIONS DES IMAGES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AIRFLOW_VERSION=2.7.0
KAFKA_VERSION=7.4.0
SPARK_VERSION=3.5.3
POSTGRES_VERSION=15
CASSANDRA_VERSION=4.1
GRAFANA_VERSION=10.1.0
PROMETHEUS_VERSION=2.47.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    CREDENTIALS (Ã  changer en production!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=pipeline_db

AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin

GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    CONFIGURATION KAFKA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KAFKA_TOPIC=my-data-topic
KAFKA_PARTITIONS=3
KAFKA_REPLICATION_FACTOR=1
```

---

## 6. âš™ï¸ Configuration de chaque Composant

### PostgreSQL - Base de DonnÃ©es SQL

```sql
-- config/init-db.sql
-- Ce script s'exÃ©cute automatiquement au premier dÃ©marrage

-- CrÃ©er la base de donnÃ©es
CREATE DATABASE pipeline_db;

-- Se connecter Ã  la base
\c pipeline_db;

-- CrÃ©er une table exemple
CREATE TABLE IF NOT EXISTS events (
    id SERIAL PRIMARY KEY,
    event_type VARCHAR(50) NOT NULL,
    event_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- CrÃ©er des index pour la performance
CREATE INDEX idx_events_type ON events(event_type);
CREATE INDEX idx_events_created ON events(created_at);

-- Donner les permissions
GRANT ALL PRIVILEGES ON DATABASE pipeline_db TO airflow;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
```

### Kafka - Configuration

```yaml
# Pas de fichier de config externe nÃ©cessaire pour dÃ©marrer
# Tout est dans docker-compose.yml via les variables d'environnement

# Mais voici les paramÃ¨tres importants Ã  connaÃ®tre:

# KAFKA_BROKER_ID: Identifiant unique du broker (1, 2, 3...)
# KAFKA_ZOOKEEPER_CONNECT: Adresse de ZooKeeper
# KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: Protocoles de sÃ©curitÃ©
# KAFKA_ADVERTISED_LISTENERS: Comment Kafka s'annonce
# KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: RÃ©plication (1 pour dev)
# KAFKA_AUTO_CREATE_TOPICS_ENABLE: CrÃ©er les topics automatiquement
```

### Prometheus - Monitoring

```yaml
# config/prometheus.yml

global:
  scrape_interval: 15s      # Collecter les mÃ©triques toutes les 15s
  evaluation_interval: 15s  # Ã‰valuer les rÃ¨gles toutes les 15s

scrape_configs:
  # Prometheus lui-mÃªme
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Kafka Exporter (mÃ©triques Kafka)
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-exporter:9308']
    
  # Node Exporter (mÃ©triques systÃ¨me) - optionnel
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

### Grafana - Datasources

```yaml
# config/grafana/provisioning/datasources/datasources.yml

apiVersion: 1

datasources:
  # Prometheus pour les mÃ©triques
  - name: Prometheus
    type: prometheus
    uid: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false

  # PostgreSQL pour les donnÃ©es
  - name: PostgreSQL
    type: postgres
    uid: postgres
    url: postgres:5432
    database: pipeline_db
    user: airflow
    secureJsonData:
      password: airflow
    jsonData:
      sslmode: disable
    editable: true
```

---

## 7. ğŸ“ Docker Compose - Le Chef d'Orchestre

### docker-compose.yml Complet

```yaml
# docker-compose.yml

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEMPLATE AIRFLOW (rÃ©utilisable)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
x-airflow-common: &airflow-common
  image: apache/airflow:${AIRFLOW_VERSION:-2.7.0}-python3.10
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: 'requests psycopg2-binary confluent-kafka'
  volumes:
    - ./dags:/opt/airflow/dags
    - ./scripts:/opt/airflow/scripts
    - airflow-logs:/opt/airflow/logs
  depends_on:
    postgres:
      condition: service_healthy
  networks:
    - pipeline-network


services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    POSTGRESQL - Base de donnÃ©es
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  postgres:
    image: postgres:${POSTGRES_VERSION:-15}
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    AIRFLOW - Orchestration
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    ZOOKEEPER - Coordination Kafka
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  zookeeper:
    image: confluentinc/cp-zookeeper:${KAFKA_VERSION:-7.4.0}
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ['CMD', 'bash', '-c', "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    KAFKA - Message Broker
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  kafka:
    image: confluentinc/cp-kafka:${KAFKA_VERSION:-7.4.0}
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    healthcheck:
      test: ["CMD", "bash", "-c", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    SPARK - Traitement DistribuÃ©
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  spark-master:
    image: apache/spark:${SPARK_VERSION:-3.5.3}
    container_name: spark-master
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"   # Spark UI
      - "7077:7077"   # Spark Master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./spark:/opt/spark-apps
    networks:
      - pipeline-network

  spark-worker:
    image: apache/spark:${SPARK_VERSION:-3.5.3}
    container_name: spark-worker
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./spark:/opt/spark-apps
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    MONITORING - Kafka UI
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    MONITORING - Prometheus
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  prometheus:
    image: prom/prometheus:v${PROMETHEUS_VERSION:-2.47.0}
    container_name: prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    MONITORING - Grafana
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-10.1.0}
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks:
      - pipeline-network


  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  #                    KAFKA EXPORTER (MÃ©triques)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    ports:
      - "9308:9308"
    command:
      - '--kafka.server=kafka:29092'
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - pipeline-network


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    VOLUMES PERSISTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
volumes:
  postgres-data:       # DonnÃ©es PostgreSQL
  airflow-logs:        # Logs Airflow
  prometheus-data:     # MÃ©triques Prometheus
  grafana-data:        # Dashboards Grafana


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    RÃ‰SEAU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
networks:
  pipeline-network:
    driver: bridge
```

### Tableau des Ports

| Service | Port Local | Port Container | URL |
|---------|------------|----------------|-----|
| **Airflow** | 8080 | 8080 | http://localhost:8080 |
| **Kafka UI** | 8081 | 8080 | http://localhost:8081 |
| **Spark Master** | 9090 | 8080 | http://localhost:9090 |
| **Spark Worker** | 8082 | 8081 | http://localhost:8082 |
| **Prometheus** | 9091 | 9090 | http://localhost:9091 |
| **Grafana** | 3000 | 3000 | http://localhost:3000 |
| **PostgreSQL** | 5432 | 5432 | localhost:5432 |
| **Kafka** | 9092 | 9092 | localhost:9092 |
| **ZooKeeper** | 2181 | 2181 | localhost:2181 |

---

## 8. â–¶ï¸ DÃ©marrer et Tester

### Ã‰tape 1 : DÃ©marrer le Pipeline

```bash
# Aller dans le dossier du projet
cd mon-premier-pipeline

# DÃ©marrer tous les services en arriÃ¨re-plan
docker-compose up -d

# Suivre les logs (optionnel)
docker-compose logs -f
```

### Ã‰tape 2 : VÃ©rifier que tout fonctionne

```bash
# VÃ©rifier le statut de tous les services
docker-compose ps

# RÃ©sultat attendu: tous les services "running" ou "healthy"
```

### Ã‰tape 3 : VÃ©rifier chaque Service

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEST POSTGRESQL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
docker exec -it postgres psql -U airflow -d pipeline_db -c "SELECT 1;"
# RÃ©sultat attendu: 1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEST KAFKA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Lister les topics
docker exec -it kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# CrÃ©er un topic de test
docker exec -it kafka kafka-topics.sh --create \
  --topic test-topic \
  --partitions 3 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092

# Envoyer un message
docker exec -it kafka bash -c "echo 'Hello World' | kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic"

# Lire les messages
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic test-topic \
  --from-beginning \
  --max-messages 1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEST SPARK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VÃ©rifier que le worker est connectÃ©
curl http://localhost:9090/json/ | jq '.workers'

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEST AIRFLOW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ouvrir http://localhost:8080
# Login: admin / admin

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    TEST GRAFANA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ouvrir http://localhost:3000
# Login: admin / admin
```

### Script de Test Automatique

```python
# scripts/test_pipeline.py
"""
Script pour tester que tous les composants du pipeline fonctionnent
"""

import subprocess
import sys
import time
import requests

def test_service(name, test_func):
    """Teste un service et affiche le rÃ©sultat"""
    try:
        result = test_func()
        print(f"âœ… {name}: OK")
        return True
    except Exception as e:
        print(f"âŒ {name}: FAILED - {e}")
        return False

def test_postgres():
    """Test PostgreSQL"""
    result = subprocess.run([
        "docker", "exec", "postgres", 
        "psql", "-U", "airflow", "-d", "pipeline_db", "-c", "SELECT 1;"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(result.stderr)
    return True

def test_kafka():
    """Test Kafka"""
    result = subprocess.run([
        "docker", "exec", "kafka", 
        "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(result.stderr)
    return True

def test_airflow():
    """Test Airflow"""
    response = requests.get("http://localhost:8080/health", timeout=10)
    if response.status_code != 200:
        raise Exception(f"Status code: {response.status_code}")
    return True

def test_spark():
    """Test Spark"""
    response = requests.get("http://localhost:9090/json/", timeout=10)
    if response.status_code != 200:
        raise Exception(f"Status code: {response.status_code}")
    return True

def test_grafana():
    """Test Grafana"""
    response = requests.get("http://localhost:3000/api/health", timeout=10)
    if response.status_code != 200:
        raise Exception(f"Status code: {response.status_code}")
    return True

def main():
    print("=" * 60)
    print("ğŸ§ª TEST DU PIPELINE BIG DATA")
    print("=" * 60)
    print()
    
    tests = [
        ("PostgreSQL", test_postgres),
        ("Kafka", test_kafka),
        ("Airflow", test_airflow),
        ("Spark Master", test_spark),
        ("Grafana", test_grafana),
    ]
    
    results = []
    for name, test_func in tests:
        results.append(test_service(name, test_func))
        time.sleep(1)
    
    print()
    print("=" * 60)
    passed = sum(results)
    total = len(results)
    
    if passed == total:
        print(f"âœ… TOUS LES TESTS PASSÃ‰S ({passed}/{total})")
    else:
        print(f"âš ï¸  {total - passed} TEST(S) Ã‰CHOUÃ‰(S) ({passed}/{total})")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## 9. ğŸ”§ Debugging et Troubleshooting

### ProblÃ¨mes Courants et Solutions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROBLÃˆMES COURANTS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: Container ne dÃ©marre pas                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. VÃ©rifier les logs: docker-compose logs nom-service                  â”‚
â”‚  2. VÃ©rifier les ports: netstat -tulpn | grep PORT                     â”‚
â”‚  3. RedÃ©marrer: docker-compose restart nom-service                     â”‚
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: "Port already in use"                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. Trouver le processus: lsof -i :8080                                â”‚
â”‚  2. Tuer le processus: kill -9 PID                                     â”‚
â”‚  3. Ou changer le port dans docker-compose.yml                         â”‚
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: Kafka ne dÃ©marre pas                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. ZooKeeper doit Ãªtre "healthy" d'abord                              â”‚
â”‚  2. docker-compose restart zookeeper kafka                             â”‚
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: Spark worker non connectÃ©                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. VÃ©rifier que spark-master est UP                                   â”‚
â”‚  2. docker-compose restart spark-worker                                â”‚
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: "Out of memory"                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. Augmenter la RAM dans Docker Desktop                               â”‚
â”‚  2. RÃ©duire les services (commenter ceux non utilisÃ©s)                 â”‚
â”‚  3. docker system prune -a (nettoyer)                                  â”‚
â”‚                                                                          â”‚
â”‚  PROBLÃˆME: Airflow DAG non visible                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  1. VÃ©rifier la syntaxe: python dags/mon_dag.py                        â”‚
â”‚  2. VÃ©rifier les logs: docker-compose logs airflow-scheduler           â”‚
â”‚  3. Attendre ~30 secondes (refresh)                                     â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Commandes de Diagnostic

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    DIAGNOSTIC GÃ‰NÃ‰RAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Voir tous les containers (mÃªme arrÃªtÃ©s)
docker ps -a

# Voir l'utilisation des ressources
docker stats --no-stream

# Voir les logs des 100 derniÃ¨res lignes
docker-compose logs --tail=100 nom-service

# Voir les logs en temps rÃ©el
docker-compose logs -f nom-service

# Inspecter un container
docker inspect nom-container

# Voir les Ã©vÃ©nements Docker
docker events --since 10m


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                    NETTOYAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ArrÃªter tous les containers
docker-compose down

# ArrÃªter et supprimer les volumes (RESET COMPLET)
docker-compose down -v

# Supprimer les containers orphelins
docker container prune

# Supprimer les images non utilisÃ©es
docker image prune -a

# Supprimer les volumes non utilisÃ©s
docker volume prune

# NETTOYAGE COMPLET (attention!)
docker system prune -a --volumes
```

### Ordre de DÃ©marrage

Si les services ne dÃ©marrent pas correctement, respecter cet ordre:

```bash
# 1. Infrastructure de base
docker-compose up -d postgres zookeeper

# Attendre 30 secondes
sleep 30

# 2. Kafka (dÃ©pend de ZooKeeper)
docker-compose up -d kafka

# Attendre 30 secondes
sleep 30

# 3. Airflow (dÃ©pend de PostgreSQL)
docker-compose up -d airflow-init
docker-compose up -d airflow-webserver airflow-scheduler

# 4. Spark
docker-compose up -d spark-master spark-worker

# 5. Monitoring
docker-compose up -d kafka-ui kafka-exporter prometheus grafana
```

---

## 10. âœ… Bonnes Pratiques

### Organisation du Code

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BONNES PRATIQUES                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  STRUCTURE:                                                             â”‚
â”‚  â–¡ Un fichier docker-compose.yml par environnement (dev, prod)          â”‚
â”‚  â–¡ Variables dans .env (jamais de secrets en dur!)                     â”‚
â”‚  â–¡ Un README.md avec instructions claires                              â”‚
â”‚  â–¡ Fichiers de config dans dossiers dÃ©diÃ©s                             â”‚
â”‚                                                                          â”‚
â”‚  DOCKER:                                                                â”‚
â”‚  â–¡ Utiliser des versions spÃ©cifiques (pas :latest en prod)             â”‚
â”‚  â–¡ Ajouter des healthchecks Ã  tous les services                        â”‚
â”‚  â–¡ DÃ©finir les dÃ©pendances (depends_on + condition)                    â”‚
â”‚  â–¡ Limiter les ressources (mem_limit, cpus)                            â”‚
â”‚  â–¡ Utiliser des volumes nommÃ©s (pas des bind mounts en prod)          â”‚
â”‚                                                                          â”‚
â”‚  KAFKA:                                                                 â”‚
â”‚  â–¡ Partitions = 3 minimum pour la production                           â”‚
â”‚  â–¡ Replication factor = 3 en production                                â”‚
â”‚  â–¡ Monitorer le consumer lag                                           â”‚
â”‚                                                                          â”‚
â”‚  SPARK:                                                                 â”‚
â”‚  â–¡ Configurer executor.memory selon les donnÃ©es                        â”‚
â”‚  â–¡ Monitorer via Spark UI                                              â”‚
â”‚  â–¡ Utiliser Parquet pour le stockage                                   â”‚
â”‚                                                                          â”‚
â”‚  AIRFLOW:                                                               â”‚
â”‚  â–¡ Un DAG par pipeline logique                                         â”‚
â”‚  â–¡ Utiliser des connections (pas de credentials en dur)                â”‚
â”‚  â–¡ Tester les DAGs localement avant dÃ©ploiement                        â”‚
â”‚  â–¡ catchup=False pour Ã©viter les exÃ©cutions massives                   â”‚
â”‚                                                                          â”‚
â”‚  MONITORING:                                                            â”‚
â”‚  â–¡ Configurer des alertes sur les mÃ©triques critiques                  â”‚
â”‚  â–¡ Dashboard Grafana pour chaque composant                             â”‚
â”‚  â–¡ Centraliser les logs                                                â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Checklist SÃ©curitÃ©

```
â–¡ Changer les mots de passe par dÃ©faut
â–¡ Ne JAMAIS commiter le fichier .env avec des secrets
â–¡ Utiliser un gestionnaire de secrets (Vault, AWS Secrets Manager)
â–¡ Limiter l'accÃ¨s rÃ©seau (firewall, VPN)
â–¡ Activer SSL/TLS pour les connexions
â–¡ Mettre Ã  jour rÃ©guliÃ¨rement les images Docker
```

---

## 11. ğŸ“‹ Checklist de DÃ©marrage

### Pour chaque nouveau projet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CHECKLIST - NOUVEAU PIPELINE BIG DATA                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  PRÃ‰PARATION:                                                           â”‚
â”‚  â–¡ Docker installÃ© et fonctionnel                                       â”‚
â”‚  â–¡ Au moins 8GB RAM disponible                                          â”‚
â”‚  â–¡ Ports requis libres (8080, 9092, etc.)                              â”‚
â”‚                                                                          â”‚
â”‚  CRÃ‰ATION DU PROJET:                                                    â”‚
â”‚  â–¡ CrÃ©er la structure de dossiers                                       â”‚
â”‚  â–¡ CrÃ©er docker-compose.yml                                             â”‚
â”‚  â–¡ CrÃ©er .env avec les variables                                        â”‚
â”‚  â–¡ CrÃ©er les fichiers de configuration                                  â”‚
â”‚                                                                          â”‚
â”‚  DÃ‰MARRAGE:                                                             â”‚
â”‚  â–¡ docker-compose up -d                                                 â”‚
â”‚  â–¡ VÃ©rifier: docker-compose ps                                          â”‚
â”‚  â–¡ Tester chaque service individuellement                               â”‚
â”‚                                                                          â”‚
â”‚  CONFIGURATION AIRFLOW:                                                 â”‚
â”‚  â–¡ AccÃ©der Ã  http://localhost:8080                                      â”‚
â”‚  â–¡ CrÃ©er les Connections nÃ©cessaires                                    â”‚
â”‚  â–¡ Activer les DAGs                                                     â”‚
â”‚                                                                          â”‚
â”‚  CONFIGURATION GRAFANA:                                                 â”‚
â”‚  â–¡ AccÃ©der Ã  http://localhost:3000                                      â”‚
â”‚  â–¡ VÃ©rifier les datasources                                             â”‚
â”‚  â–¡ Importer/crÃ©er les dashboards                                        â”‚
â”‚                                                                          â”‚
â”‚  TESTS:                                                                 â”‚
â”‚  â–¡ Tester l'envoi de messages Kafka                                     â”‚
â”‚  â–¡ Tester une requÃªte PostgreSQL                                        â”‚
â”‚  â–¡ Lancer un job Spark de test                                          â”‚
â”‚  â–¡ VÃ©rifier les mÃ©triques dans Grafana                                  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ RÃ©sumÃ©

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PREMIER PIPELINE - EN BREF                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                            â•‘
â•‘  ARCHITECTURE:  Source â†’ Kafka â†’ Spark â†’ Database â†’ Grafana               â•‘
â•‘                                                                            â•‘
â•‘  DOCKER:        docker-compose up -d    (dÃ©marrer)                        â•‘
â•‘                 docker-compose down     (arrÃªter)                         â•‘
â•‘                 docker-compose logs -f  (voir les logs)                   â•‘
â•‘                                                                            â•‘
â•‘  PORTS:         8080 = Airflow                                            â•‘
â•‘                 8081 = Kafka UI                                           â•‘
â•‘                 9090 = Spark                                              â•‘
â•‘                 3000 = Grafana                                            â•‘
â•‘                 9092 = Kafka                                              â•‘
â•‘                 5432 = PostgreSQL                                         â•‘
â•‘                                                                            â•‘
â•‘  FICHIERS:      docker-compose.yml = tous les services                   â•‘
â•‘                 .env = variables d'environnement                          â•‘
â•‘                 config/ = configurations                                  â•‘
â•‘                 dags/ = jobs Airflow                                      â•‘
â•‘                 spark/ = scripts Spark                                    â•‘
â•‘                                                                            â•‘
â•‘  DEBUG:         docker-compose ps (statut)                                â•‘
â•‘                 docker-compose logs nom-service (logs)                    â•‘
â•‘                 docker exec -it nom-container bash (entrer)               â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

> **Prochaine Ã©tape** : CrÃ©er ton premier DAG Airflow qui envoie des donnÃ©es vers Kafka ! ğŸš€